{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5985adc8-94c4-4014-88d4-815c8b94be2d",
   "metadata": {},
   "source": [
    "# Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7365df75-cd63-4067-809c-4498ab6410cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f39d22-69e9-4909-bbae-cfc2f9baeced",
   "metadata": {},
   "source": [
    "# Required lib & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b739775e-7a51-41ca-bf57-c9b75f252ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad7bf22-29d7-4b22-aac8-24024d4902c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3085e5cb-22cf-433e-a3d4-eaab5570e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re  # Regular expressions for text cleaning\n",
    "import nltk  # NLP library\n",
    "nltk.download('punkt')  # Download tokenizer\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69d0df-dd4f-4329-b69a-93bb9b2a4a30",
   "metadata": {},
   "source": [
    "# Decontraction of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f071a13-695a-4b5d-8cfd-7c40b53a13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontract(sentence):\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "510d18f1-6b69-4b79-a1d8-39f3b4b5fb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are working very hard\n"
     ]
    }
   ],
   "source": [
    "text=\"They're working very hard\"\n",
    "print(decontract(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7c6d4-c111-41ab-98f5-f5ab4c7df1b1",
   "metadata": {},
   "source": [
    "# Removing number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031aeaba-f831-4638-b3b4-e03813270b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  people working in that area \n"
     ]
    }
   ],
   "source": [
    "def removeNumber(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', '', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    return alpha_sent\n",
    "\n",
    "text=\"There are 5 people working in that area\"\n",
    "print(removeNumber(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08feb1-3285-4b11-876e-19365521e6e8",
   "metadata": {},
   "source": [
    "# Removing URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c776df6f-b3e4-4467-a1d1-31697ba35385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my website http://towardsdeeplearning.com/sentiment-analysis-using-lstm-and-glove-embeddings-993343a87fe7e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is my website '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='This is my website http://towardsdeeplearning.com/sentiment-analysis-using-lstm-and-glove-embeddings-993343a87fe7e'\n",
    "text=str(text)\n",
    "#text.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n",
    "print(text)\n",
    "res=re.sub(r'http\\S+', '', text)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362d60b-7566-4649-a337-9e041eb2319d",
   "metadata": {},
   "source": [
    "# Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3454c6f9-793a-4f72-b169-461f188a7d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'httptowardsdeeplearningcomsentimentanalysisusinglstmandgloveembeddings993343a87fe7e  '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "text='http://towardsdeeplearning.com/sentiment-analysis-using-lstm-and-glove-embeddings-993343a87fe7e  .,;:!@#$?'\n",
    "text = text.translate(str.maketrans(' ', ' ', PUNCT_TO_REMOVE))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc617c-9cbd-46b3-aa4a-f3dedc2a51a1",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621ca944-ae50-4fea-a577-a74999f719d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going travelling winter !'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "text = \"we are going to travelling in winter !\"\n",
    "# removing stopwords\n",
    "text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b6439-0b56-49cd-afad-ff1f3be2b01a",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "983d1b06-540a-4990-8fa6-9a7c22ce429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going travelling winter !\n",
      "walk ---> walk\n",
      "walking ---> walking\n",
      "walked ---> walked\n",
      "walks ---> walk\n",
      "ran ---> ran\n",
      "run ---> run\n",
      "running ---> running\n",
      "runs ---> run\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "print(text)\n",
    "\n",
    "words = [\"walk\", \"walking\", \"walked\", \"walks\", \"ran\", \"run\", \"running\", \"runs\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word + \" ---> \" + lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2f99a-55d3-45db-bd6f-892664e70963",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e9bfb1c-bf9f-4d19-a2f8-7efb1e782963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk ---> walk\n",
      "walking ---> walk\n",
      "walked ---> walk\n",
      "walks ---> walk\n",
      "ran ---> ran\n",
      "run ---> run\n",
      "running ---> run\n",
      "runs ---> run\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "for word in words: \n",
    "    print(word + \" ---> \" + stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e509f-93e6-4d89-bd06-3911262ea8da",
   "metadata": {},
   "source": [
    "# lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f8ff232-72b7-4523-8dab-f0232cf3365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, i am snehal. how are you?  \n"
     ]
    }
   ],
   "source": [
    "text = \"HELLO, I am Snehal. How are you?  \" \n",
    "\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31f8e6-76bd-405e-ad24-b3e0a6bf601d",
   "metadata": {},
   "source": [
    "# Removing mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1e1041b-69ab-4012-8cb4-927701e2bc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    are now on Linkedin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\sneha\\AppData\\Local\\Temp\\ipykernel_27824\\228757596.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  x = re.sub(\"@\\S+\", \" \", x)\n"
     ]
    }
   ],
   "source": [
    "x = \"@Snehalkhandare, @akanksha are now on Linkedin\"\n",
    "x = re.sub(\"@\\S+\", \" \", x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8789258-0073-407f-bd9a-250cffbe389d",
   "metadata": {},
   "source": [
    "# URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e662acf3-1553-424d-a670-4df8b1de4e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is url of my blog  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\sneha\\AppData\\Local\\Temp\\ipykernel_27824\\2741156097.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  x = re.sub(\"https*\\S+\", \" \", x)\n"
     ]
    }
   ],
   "source": [
    "x = \"This is url of my blog http://t.co/UZWZgJQzPQ\"\n",
    "x = re.sub(\"https*\\S+\", \" \", x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d31bc1-f976-4d97-9cd3-f803e6ef35b2",
   "metadata": {},
   "source": [
    "# Removing Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55f113c5-bb65-441c-b8f6-51be6d767ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are people not concerned that after   obliteration in Scotland   UK is ripping itself apart over   contest?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\sneha\\AppData\\Local\\Temp\\ipykernel_27824\\771092978.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  x = re.sub(\"#\\S+\", \" \", x)\n"
     ]
    }
   ],
   "source": [
    "x = \"Are people not concerned that after #SLAB's obliteration in Scotland #Labour UK is ripping itself apart over #Labourleadership contest?\"\n",
    "x = re.sub(\"#\\S+\", \" \", x)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
